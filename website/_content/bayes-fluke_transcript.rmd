
Whenever we collect data, there is some element of chance. 
If the data we have are limited, it's possible that any patterns have only occurred by chance.
We need to be able to quantify how much information our data provide --- that is, 
how much more or less confident about the patterns we are after we make observations.


### Bayes Factors for *differences*

Previously, we emphasised that summaries and descriptions of data are used *to answer research questions*.

For example, we used a boxplot to compare the differences in the amount of weight participants lost when
treated with either Functional Imagery Training or Motivational Interviewing (the `funimagery` dataset). 
This lets us answer the question: *"which intervention was more effective in helping participants lose weight?"*:

```{r, echo=F}
funimagery %>%
  ggplot(aes(intervention, weight_lost_end_trt)) +
  geom_boxplot()
```

```{r, echo=F}
fitdiff <- funimagery %>% group_by(intervention) %>% summarise(m=mean(weight_lost_end_trt)) %>% pull(m) %>% diff %>% round(1) %>% abs
```


This boxplot shows the *median* and *interquartile range* for each treatment group. It's immediately obvious
that --- *in this sample* --- there is a difference between the two groups (`r fitdiff` kg, in fact), and that --- *in this sample* --- FIT helped people lose more weight than MI.

For many purposes this plot is enough.

The difference here is quite large and we can interpret this graph at face value. If the costs of these interventions were similar (i.e. if 'all other things are equal') it we should choose FIT rather than MI.

However, researchers often want to *quantify the evidence* provided by the sample of data they collect. 
This is important in psychology, and other sciences, because the size of the effects we see are often quite small or subtle.

It *might* be the case that patterns we see are just a 'fluke'. That is, they have come about by chance. Perhaps if we took another sample from the same population (treated more people with FIT and MI) then we would see no difference?


The Bayes Factor quantifies how confident we should be about any given pattern in our by comparing two possibilities:

1. There really is a difference in how much weight people lost with FIT and MI (we call this H1)
2. There's NO difference between FIT and MI; any difference we see is likely to be a fluke (H0)



### Bayes Factors for *correlations*

We have also seen how a scatterplot describes the *relationship* between two columns of data:

```{r, echo=F}
fuel %>%
  ggplot(aes(engine_size, power)) + geom_point()
```
```{r, echo=F, message=F, warning=F}
enginecor <- fuel %>% select(engine_size, power) %>% correlate() %>% pull(power) %>% first %>% round(2)
```


From this plot the relationship seems quite clear, and the correlation between these columns is `r enginecor`

However, we don't have that many data points, so the pattern _might_ have happened by chance.

The Bayes Factor allows us to quantify how much evidence we have for this correlation. As before, we are
comparing two different possibilities:

1. There is a correlation between `engine_size` and `power`
2. There is **NO** correlation (the pattern is just a fluke)



